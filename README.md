# Haskell parsing benchmarks

A simple benchmark comparing the performance of different parser
implementations. The objective is to parse the following grammar:

```bnf
SUM   = SUM '+' PROD
      | SUM '-' PROD
      | PROD

PROD  = PROD '*' number
      | PROD '/' number
      | number
```

There are three parsers implemented right now:
- __Handwritten__: The parser is implemented using a CPS monad (called
  `Lexer`), managing the remaining input, the next token, the source
  position, and the failure state. Even though the source position is
  not used in this example, it is still computed because it's a very
  common requirement for a parser.

- __Attoparsec__: Your typical parser using parser-combinators. Pretty
  similar to the handwritten parser, except that we backtrack when
  an unexpected token is encountered, unlike above, where we decide
  whether to shift or reduce, based on the next token.

- __Alex/Happy__: An LALR(1) parser, automatically generated by
  _Happy_, and a lexer, automatically generated by _Alex_.

## Benchmark

Parse the file generated by `gen-example.py`. It is roughly 5MB in
size and contains around 1 million numbers, each having 1 to 4 digits,
seperated by a randomly chosen operator (`+`, `-`, `*`, `/`).

Reading the file is part of the benchmark, since I would consider this
part of the parser.

## Results

The benchmark was compiled with GHC 9.4.2, without a threaded runtime
or the LLVM code generator, but with `-O2`.

| Parser      | Time      | Factor |
|:----------- |:--------- |:------ |
| Handwritten | 192  ms   | 1.00   |
| Attoparsec  | 337  ms   | 1.75   |
| Alex/Happy  | 1.03 s    | 5.36   |

## Running it yourself
```sh
$ python ./gen-example.py
$ cabal bench
```
If you want experiment with the different implementations, run

```sh
$ cabal bench --benchmark-options="--csv baseline.csv"
```

make the changes you want to make, and then run

```sh
$ cabal bench --benchmark-options="--baseline baseline.csv"
```

to see how much the performance changed.
