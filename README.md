# Haskell parsing benchmarks

A simple benchmark comparing the performance of different parser
implementations. The objective is to parse the following grammar:

```bnf
SUM   = SUM '+' PROD
      | SUM '-' PROD
      | PROD

PROD  = PROD '*' ATOM
      | PROD '/' ATOM
      | ATOM

ATOM  = integer
      | '(' SUM ')'
```

The following parsers are implemented right now:
- __Handwritten__: A handwritten lexer and recursive ascent parser. Even though
  the source position is not used in this example, it is still
  computed because it's a very common requirement for a parser. Only
  ByteString is supported.

- __Attoparsec__: A parser using the parser combinator library
  [Attoparsec](https://hackage.haskell.org/package/attoparsec). ByteString
  and Text are supported.

- __Megaparsec__: A parser using the parser combinator library
  [Megaparsec](https://hackage.haskell.org/package/megaparsec). ByteString
  and Text are supported.

- __Parsec__: A parser using the parser combinator library
  [Parsec](https://hackage.haskell.org/package/parsec). ByteString and
  Text are supported.

- __Flatparse__: A parser using the comparatively new parser
  combinator library
  [Flatparse](https://hackage.haskell.org/package/flatparse). Only
  ByteString is supported.

- __UU Parsing Lib__: A parser using the parser combinator library
  [uu-parsinglib](https://hackage.haskell.org/package/uu-parsinglib). Only
  Text is supported.

- __Alex/Happy__: An LALR(1) parser, automatically generated by
  _Happy_, and a lexer, automatically generated by _Alex_. Only
  ByteString is supported.

- __Parsley__: A parser using the Template Haskell-driven parser
  combinator library
  [Parsley](https://hackage.haskell.org/package/parsley). ByteString
  and Text are supported.

## Benchmark

Parse the file generated by `gen-example.py`. It is roughly 5MB in
size and contains around 1 million numbers, each having 1 to 4 digits,
separated by a randomly chosen operator (`+`, `-`, `*`,
`/`). Parenthesis are randomly inserted.

Reading the file is part of the benchmark, since I would consider this
part of the parser.

## Results

| Parser                      | Time      | Factor | Memory allocated |
|:--------------------------- | ---------:| ------:| ----------------:|
| Flatparse                   | 208  ms   | 1.00x  | 70 MB            |
| Handwritten                 | 229  ms   | 1.10x  | 292 MB           |
| Parsley (ByteString)        | 339  ms   | 1.63x  | 806 MB           |
| Attoparsec (ByteString)     | 360  ms   | 1.73x  | 1.3 GB           |
| Parsley (Text)              | 377  ms   | 1.81x  | 885 MB           |
| Attoparsec (Text)           | 392  ms   | 1.88x  | 1.3 GB           |
| Megaparsec (ByteString)     | 471  ms   | 2.26x  | 2.3 GB           |
| Megaparsec (Text)           | 620  ms   | 2.98x  | 3.0 GB           |
| Alex/Happy                  | 924  ms   | 4.43x  | 2.8 GB           |
| Parsec (ByteString)         | 1.50  s   | 7.24x  | 6.4 GB           |
| Parsec (Text)               | 1.52  s   | 7.30x  | 6.4 GB           |
| UU Parsing Lib (ByteString) | 3.81  s   | 18.28x | 5.5 GB           |

_Note: Memory allocated is not peak memory consumption, since it does
not decrease, when memory is freed._

The benchmark was compiled with GHC 9.2.1, without a threaded runtime
or the LLVM code generator, but with `-O2`.

## Notes

_Flatparse_, _Attoparsec_, _Megaparsec_, and _Parsley_ benefit greatly
from the `Strict` GHC extension, as they run twice as fast. _Parsec_
is not affected by `Strict`. The handwritten parser performs best with
`StrictData`. All implementations suffer from at least a 2x slowdown
when compiled with `-threaded` and run with `+RTS -N`.

I did try benchmarking
[Earley](https://hackage.haskell.org/package/Earley), but on a file
this size it consumed multiple gigabytes of memory. On smaller files
it was around 200x slower than _Flatparse_. I did however not use a
tokenizer, which could have improved its performance.

## Running it yourself

```sh
$ python ./gen-example.py
$ cabal bench
```

If you want to make changes, you should run

```sh
$ cabal bench --benchmark-options="--csv baseline.csv"
```

once. Make the changes you want to make, and then run

```sh
$ cabal bench --benchmark-options="--baseline baseline.csv"
```

to see how much the performance has changed.


## Credits

| Name            | Contribution                   |
|:--------------- |:------------------------------ |
| Jaro Reinders   | UU Parsing Lib benchmark       |
